#!/usr/bin/env python3
"""
Analysis tool for Batch 8 (iterations 29-32).

Key questions to answer:
1. WHY did n_layers=4 help Model 011 but NOT Model 049?
2. WHY did n_layers_update=4 hurt Model 011 so much?
3. WHY is lr_W=4E-4 the sweet spot for Model 041 (not 3E-4)?
4. Confirm n_layers=4 is neutral for SOLVED Model 003.

Model configurations tested:
- Model 049 (Iter 29): n_layers=4 + embedding_dim=4 (REGRESSED: 0.181->0.166)
- Model 011 (Iter 30): n_layers_update=4 (CATASTROPHIC: 0.769->0.620)
- Model 041 (Iter 31): lr_W=3E-4 (REGRESSED: 0.919->0.888)
- Model 003 (Iter 32): n_layers=4 + embedding_dim=4 (STABLE: 0.967)
"""

import numpy as np
import torch
import os

# Model mappings
MODEL_IDS = ['049', '011', '041', '003']
SLOTS = [0, 1, 2, 3]
SLOT_TO_MODEL = dict(zip(SLOTS, MODEL_IDS))

def load_model_state(slot):
    """Load trained model state dict for a slot."""
    path = f'log/fly/fly_N9_62_1_understand_Claude_{slot:02d}/models/best_model_with_0_graphs_0.pt'
    try:
        sd = torch.load(path, map_location='cpu', weights_only=False)
        return sd['model_state_dict']
    except Exception as e:
        print(f"  Error loading slot {slot}: {e}")
        return None

def load_ground_truth(model_id):
    """Load ground truth W for a model."""
    path = f'graphs_data/fly/fly_N9_62_1_id_{model_id}/weights.pt'
    try:
        W = torch.load(path, map_location='cpu', weights_only=True).numpy()
        return W
    except Exception as e:
        print(f"  Error loading W for model {model_id}: {e}")
        return None

def load_edge_index(model_id):
    """Load edge index for a model."""
    path = f'graphs_data/fly/fly_N9_62_1_id_{model_id}/edge_index.pt'
    try:
        E = torch.load(path, map_location='cpu', weights_only=True).numpy()
        return E
    except Exception as e:
        print(f"  Error loading edge_index for model {model_id}: {e}")
        return None

def analyze_mlp_structure(state_dict, prefix, n_layers):
    """Analyze MLP layer structure and weight statistics."""
    layer_stats = []
    for i in range(n_layers):
        w_key = f'{prefix}.layers.{i}.weight'
        b_key = f'{prefix}.layers.{i}.bias'
        if w_key in state_dict:
            w = state_dict[w_key].numpy()
            b = state_dict[b_key].numpy() if b_key in state_dict else None
            stats = {
                'layer': i,
                'shape': w.shape,
                'mean': float(np.mean(w)),
                'std': float(np.std(w)),
                'min': float(np.min(w)),
                'max': float(np.max(w)),
                'frac_positive': float(np.mean(w > 0)),
                'frac_large': float(np.mean(np.abs(w) > 0.1)),
                'l2_norm': float(np.linalg.norm(w)),
            }
            layer_stats.append(stats)
    return layer_stats

def compute_w_recovery_stats(W_true, W_learned):
    """Compute W recovery statistics."""
    if W_true is None or W_learned is None:
        return None

    # Basic stats
    pearson = np.corrcoef(W_true, W_learned)[0, 1]
    ss_res = np.sum((W_true - W_learned) ** 2)
    ss_tot = np.sum((W_true - np.mean(W_true)) ** 2)
    r2 = 1 - ss_res / ss_tot if ss_tot > 0 else 0

    # Sign analysis
    sign_match = np.mean(np.sign(W_true) == np.sign(W_learned))

    # Magnitude analysis
    mag_ratio = np.mean(np.abs(W_learned)) / (np.mean(np.abs(W_true)) + 1e-10)

    return {
        'pearson': pearson,
        'r2': r2,
        'sign_match': sign_match,
        'mag_ratio': mag_ratio,
        'true_mean': float(np.mean(W_true)),
        'true_std': float(np.std(W_true)),
        'learned_mean': float(np.mean(W_learned)),
        'learned_std': float(np.std(W_learned)),
    }

def compute_per_neuron_w_stats(W_true, W_learned, edge_index, n_neurons=13741):
    """Compute per-neuron W recovery statistics (incoming and outgoing)."""
    if W_true is None or W_learned is None or edge_index is None:
        return None

    src = edge_index[0]  # source neurons
    tgt = edge_index[1]  # target neurons

    # Compute per-neuron incoming W sum (sum of weights on edges TO this neuron)
    incoming_true = np.zeros(n_neurons)
    incoming_learned = np.zeros(n_neurons)
    outgoing_true = np.zeros(n_neurons)
    outgoing_learned = np.zeros(n_neurons)

    for i in range(len(W_true)):
        s, t = src[i], tgt[i]
        incoming_true[t] += W_true[i]
        incoming_learned[t] += W_learned[i]
        outgoing_true[s] += W_true[i]
        outgoing_learned[s] += W_learned[i]

    # Correlations
    inc_pearson = np.corrcoef(incoming_true, incoming_learned)[0, 1]
    out_pearson = np.corrcoef(outgoing_true, outgoing_learned)[0, 1]

    return {
        'incoming_pearson': inc_pearson,
        'outgoing_pearson': out_pearson,
        'incoming_true_mean': float(np.mean(incoming_true)),
        'incoming_learned_mean': float(np.mean(incoming_learned)),
        'outgoing_true_mean': float(np.mean(outgoing_true)),
        'outgoing_learned_mean': float(np.mean(outgoing_learned)),
    }

def main():
    print("=" * 70)
    print("ANALYSIS: Batch 8 (Iterations 29-32) â€” Depth Experiments Results")
    print("=" * 70)

    # Configuration for each slot this batch
    configs = {
        0: {'model': '049', 'change': 'n_layers=4 + embedding_dim=4', 'n_layers': 4, 'n_layers_update': 3, 'emb_dim': 4, 'result': 'REGRESSED 0.181->0.166'},
        1: {'model': '011', 'change': 'n_layers_update=4', 'n_layers': 4, 'n_layers_update': 4, 'emb_dim': 2, 'result': 'CATASTROPHIC 0.769->0.620'},
        2: {'model': '041', 'change': 'lr_W=3E-4', 'n_layers': 3, 'n_layers_update': 3, 'emb_dim': 2, 'result': 'REGRESSED 0.919->0.888'},
        3: {'model': '003', 'change': 'n_layers=4 + embedding_dim=4', 'n_layers': 4, 'n_layers_update': 3, 'emb_dim': 4, 'result': 'STABLE 0.967'},
    }

    print("\n" + "=" * 70)
    print("1. WHY DID n_layers_update=4 HURT MODEL 011?")
    print("=" * 70)

    state_011 = load_model_state(1)  # Iter 30: n_layers_update=4

    if state_011:
        print("\n--- Model 011 (Iter 30): n_layers_update=4 ---")
        print("  Result: CATASTROPHIC regression 0.769 -> 0.620")
        print("  V_rest collapsed to 0.000")

        # Analyze lin_edge (edge MLP) - 4 layers
        edge_stats = analyze_mlp_structure(state_011, 'lin_edge', 4)
        print(f"\n  lin_edge (edge MLP, n_layers=4):")
        total_edge_params = 0
        for ls in edge_stats:
            print(f"    Layer {ls['layer']}: shape={ls['shape']}, L2={ls['l2_norm']:.3f}, frac_large={ls['frac_large']:.3f}")
            total_edge_params += np.prod(ls['shape'])
        print(f"    Total params: {total_edge_params}")

        # Analyze lin_phi (update MLP) - 4 layers
        phi_stats = analyze_mlp_structure(state_011, 'lin_phi', 4)
        print(f"\n  lin_phi (update MLP, n_layers_update=4):")
        total_phi_params = 0
        for ls in phi_stats:
            print(f"    Layer {ls['layer']}: shape={ls['shape']}, L2={ls['l2_norm']:.3f}, frac_large={ls['frac_large']:.3f}")
            total_phi_params += np.prod(ls['shape'])
        print(f"    Total params: {total_phi_params}")

        # Check W recovery
        W_true = load_ground_truth('011')
        W_learned = state_011['W'].numpy().flatten()
        E = load_edge_index('011')

        stats = compute_w_recovery_stats(W_true, W_learned)
        pn_stats = compute_per_neuron_w_stats(W_true, W_learned, E)

        if stats:
            print(f"\n  W recovery:")
            print(f"    Pearson: {stats['pearson']:.4f}, R2: {stats['r2']:.4f}")
            print(f"    Sign match: {stats['sign_match']:.3f}, Mag ratio: {stats['mag_ratio']:.3f}")

        if pn_stats:
            print(f"\n  Per-neuron W recovery:")
            print(f"    Incoming Pearson: {pn_stats['incoming_pearson']:.4f}")
            print(f"    Outgoing Pearson: {pn_stats['outgoing_pearson']:.4f}")

        print("\n  HYPOTHESIS: n_layers_update=4 increases update MLP capacity too much.")
        print("    - The update MLP learns tau/V_rest, which collapsed")
        print("    - Extra capacity in update MLP may cause overfitting or instability")
        print("    - Edge MLP depth helps W learning; update MLP depth hurts tau/V_rest")

    print("\n" + "=" * 70)
    print("2. MODEL 049 vs MODEL 003: SAME ARCHITECTURE, DIFFERENT OUTCOMES")
    print("=" * 70)

    # Both have n_layers=4 + embedding_dim=4
    state_049 = load_model_state(0)  # REGRESSED
    state_003 = load_model_state(3)  # STABLE

    if state_049 and state_003:
        print("\n  Both models: n_layers=4 + embedding_dim=4 + n_layers_update=3")
        print("  Model 049: REGRESSED 0.181 -> 0.166")
        print("  Model 003: STABLE 0.967")

        # Compare lin_edge structures
        edge_049 = analyze_mlp_structure(state_049, 'lin_edge', 4)
        edge_003 = analyze_mlp_structure(state_003, 'lin_edge', 4)

        print("\n  lin_edge comparison (output layer):")
        if edge_049 and edge_003:
            out_049 = edge_049[-1]
            out_003 = edge_003[-1]
            print(f"    Model 049: shape={out_049['shape']}, L2={out_049['l2_norm']:.3f}, frac_large={out_049['frac_large']:.3f}")
            print(f"    Model 003: shape={out_003['shape']}, L2={out_003['l2_norm']:.3f}, frac_large={out_003['frac_large']:.3f}")

        # Compare W recovery
        W_true_049 = load_ground_truth('049')
        W_true_003 = load_ground_truth('003')
        W_learned_049 = state_049['W'].numpy().flatten()
        W_learned_003 = state_003['W'].numpy().flatten()
        E_049 = load_edge_index('049')
        E_003 = load_edge_index('003')

        stats_049 = compute_w_recovery_stats(W_true_049, W_learned_049)
        stats_003 = compute_w_recovery_stats(W_true_003, W_learned_003)

        print("\n  W recovery comparison:")
        if stats_049 and stats_003:
            print(f"    Model 049: Pearson={stats_049['pearson']:.4f}, R2={stats_049['r2']:.4f}, sign_match={stats_049['sign_match']:.3f}")
            print(f"    Model 003: Pearson={stats_003['pearson']:.4f}, R2={stats_003['r2']:.4f}, sign_match={stats_003['sign_match']:.3f}")

        pn_049 = compute_per_neuron_w_stats(W_true_049, W_learned_049, E_049)
        pn_003 = compute_per_neuron_w_stats(W_true_003, W_learned_003, E_003)

        print("\n  Per-neuron W recovery comparison:")
        if pn_049 and pn_003:
            print(f"    Model 049: inc_Pearson={pn_049['incoming_pearson']:.4f}, out_Pearson={pn_049['outgoing_pearson']:.4f}")
            print(f"    Model 003: inc_Pearson={pn_003['incoming_pearson']:.4f}, out_Pearson={pn_003['outgoing_pearson']:.4f}")

        print("\n  KEY INSIGHT: SAME architecture, OPPOSITE outcomes.")
        print("    Model 003: POSITIVE per-neuron W correlation -> SOLVED")
        print("    Model 049: NEGATIVE per-neuron W correlation -> FUNDAMENTAL LIMITATION")
        print("    Architecture cannot fix Model 049's structural degeneracy.")

    print("\n" + "=" * 70)
    print("3. MODEL 041: WHY IS lr_W=4E-4 OPTIMAL, NOT 3E-4?")
    print("=" * 70)

    state_041 = load_model_state(2)  # lr_W=3E-4

    if state_041:
        W_true_041 = load_ground_truth('041')
        W_learned_041 = state_041['W'].numpy().flatten()
        E_041 = load_edge_index('041')

        stats_041 = compute_w_recovery_stats(W_true_041, W_learned_041)
        pn_041 = compute_per_neuron_w_stats(W_true_041, W_learned_041, E_041)

        print("\n--- Model 041 (Iter 31): lr_W=3E-4 ---")
        print("  Result: REGRESSED 0.919 -> 0.888")

        if stats_041:
            print(f"\n  W recovery:")
            print(f"    Pearson: {stats_041['pearson']:.4f}, R2: {stats_041['r2']:.4f}")
            print(f"    Sign match: {stats_041['sign_match']:.3f}, Mag ratio: {stats_041['mag_ratio']:.3f}")

        if pn_041:
            print(f"\n  Per-neuron W recovery:")
            print(f"    Incoming Pearson: {pn_041['incoming_pearson']:.4f}")
            print(f"    Outgoing Pearson: {pn_041['outgoing_pearson']:.4f}")

        # Analyze W distribution
        print(f"\n  W distribution:")
        print(f"    True W:    mean={np.mean(W_true_041):.6f}, std={np.std(W_true_041):.6f}")
        print(f"    Learned W: mean={np.mean(W_learned_041):.6f}, std={np.std(W_learned_041):.6f}")

        print("\n  HYPOTHESIS: lr_W=3E-4 is too slow for effective W convergence.")
        print("    - Near-collapsed activity (svd_rank=6) provides weak gradient signal")
        print("    - lr_W=4E-4 balances signal exploitation without overfitting")
        print("    - lr_W=3E-4 under-learns, lr_W=6E-4 may over-learn for connectivity")

    print("\n" + "=" * 70)
    print("4. OVERALL MODEL STATUS SUMMARY")
    print("=" * 70)

    print("""
  MODEL 049 (svd_rank=19):
    - FUNDAMENTAL LIMITATION CONFIRMED (12/12 experiments regressed)
    - n_layers=4 + embedding_dim=4 did NOT help (regression)
    - tau/V_rest excellent (0.968/0.841) but W fundamentally broken
    - Per-neuron W correlation NEGATIVE -> structural degeneracy
    - CONCLUSION: UNSOLVABLE with current GNN architecture

  MODEL 011 (svd_rank=45):
    - n_layers=4 (edge MLP) helps: 0.716 -> 0.769
    - n_layers_update=4 (update MLP) HURTS: 0.769 -> 0.620, V_rest=0
    - ONLY edge MLP depth helps; update MLP depth is harmful
    - Best config: n_layers=4 + n_layers_update=3
    - CONCLUSION: PARTIAL SOLVED (0.769 best)

  MODEL 041 (svd_rank=6):
    - lr_W=4E-4 is OPTIMAL (0.919)
    - lr_W=3E-4 is TOO SLOW (0.888)
    - Sweet spot: lr_W=4E-4 for connectivity
    - CONCLUSION: CONNECTIVITY SOLVED (0.919 best)

  MODEL 003 (svd_rank=60):
    - n_layers=4 + embedding_dim=4 is NEUTRAL (0.967)
    - 8 confirmations of SOLVED status
    - CONCLUSION: FULLY SOLVED (0.972 best)
    """)

    print("\n" + "=" * 70)
    print("5. NEW PRINCIPLES DISCOVERED")
    print("=" * 70)

    print("""
  P1. Edge MLP depth (n_layers=4) can help difficult models (Model 011)
      but NOT fundamentally broken models (Model 049).

  P2. Update MLP depth (n_layers_update=4) is HARMFUL for V_rest/tau recovery.
      Keep n_layers_update=3 regardless of edge MLP depth.

  P3. Per-neuron W correlation PREDICTS solvability:
      - POSITIVE correlation -> model is solvable
      - NEGATIVE correlation -> fundamental limitation

  P4. lr_W has a sweet spot for near-collapsed activity:
      - Model 041: lr_W=4E-4 optimal
      - lr_W=3E-4 too slow, lr_W=6E-4 may be too fast

  P5. Architecture cannot fix structural degeneracy:
      - Model 049 vs Model 003: same architecture, opposite outcomes
      - Difference is in model structure, not hyperparameters
    """)

    print("\n" + "=" * 70)
    print("6. RECOMMENDATIONS FOR BATCH 9")
    print("=" * 70)

    print("""
  Model 049:
    - Consider UNSOLVABLE. Use slot for exploratory analysis.
    - Could try: recurrent_training=True or fundamentally different approach

  Model 011:
    - Exploit n_layers=4 + n_layers_update=3
    - Try: hidden_dim=96 (more width), or W_L1=2E-5 with n_layers=4

  Model 041:
    - Connectivity SOLVED at lr_W=4E-4
    - Could try: phi_L2=0.002 + lr_W=4E-4 for better tau balance

  Model 003:
    - FULLY SOLVED. No further exploration needed.
    - Use slot as control for cross-model experiments
    """)

    print("=" * 70)
    print("END ANALYSIS")
    print("=" * 70)

if __name__ == '__main__':
    main()
