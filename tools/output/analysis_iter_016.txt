======================================================================
ANALYSIS ITER 016: Why Did Stronger Regularization FAIL for Model 049?
======================================================================

Loaded Model 049
Loaded Model 011
Loaded Model 041
Loaded Model 003

======================================================================
1. BATCH 4 RESULTS SUMMARY
======================================================================

Current Status (Batch 4):
--------------------------------------------------------------------------------
Model    conn_R2    tau_R2     V_rest_R2    Config                                  
--------------------------------------------------------------------------------
049      0.108      0.606      0.566        edge_norm=5.0, W_L1=1E-4                
011      0.544      0.221      0.001        lr_emb=2E-3 (CATASTROPHIC)              
041      0.912      0.373      0.014        edge_diff=1500 (STABLE)                 
003      0.966      0.962      0.685        Iter4 config (CONFIRMED)                

Best Results Across All Iterations:
--------------------------------------------------------------------------------
Model 049: best=0.634 (Iter baseline) - PARTIAL
Model 011: best=0.716 (Iter 2) - PARTIAL
Model 041: best=0.912 (Iter 15) - SOLVED
Model 003: best=0.972 (Iter 4) - SOLVED

======================================================================
2. MODEL 049: Why Stronger Regularization FAILED
======================================================================

Hypothesis 1: Over-regularization killed the learning signal
------------------------------------------------------------
  W_true average magnitude:    0.079866
  W_learned average magnitude: 0.128881
  Ratio (learned/true): 1.6137
  Near-zero W_learned edges (<0.001): 7788 / 434112 (1.8%)

Hypothesis 2: Sign structure analysis
------------------------------------------------------------
  Sign match rate: 19.7%
  W_true positive fraction:    0.630
  W_learned positive fraction: 0.461

Hypothesis 3: Structural sign inversion analysis
------------------------------------------------------------
  Pearson(W_true, W_learned): -0.1280
  NEGATIVE CORRELATION: Signs partially inverted

======================================================================
3. LIN_EDGE_POSITIVE CONSTRAINT ANALYSIS
======================================================================


The FlyVis GNN uses lin_edge_positive=True by default.
This means edge messages are SQUARED, forcing them to be positive.

In the PDE equation:
  tau * dv/dt = -v + V_rest + sum_j W_ij * ReLU(v_j) + I

The W_ij can be positive or negative (excitatory/inhibitory).
BUT if lin_edge outputs only positive values, how are inhibitory connections represented?

Possible mechanisms:
1. The W parameter itself encodes sign (W can be negative)
2. The lin_phi MLP compensates by learning negative coefficients
3. The embedding space encodes excitatory/inhibitory nature

For Model 049, if activity is low-rank, the GNN may find it easier to use
NEGATIVE W with positive lin_edge messages, instead of vice versa.
This would explain the sign inversion.

Lin_edge layer analysis:
  lin_edge.layers.0.weight: shape=(80, 3), positive_frac=0.45, mean=-0.0178
  lin_edge.layers.1.weight: shape=(80, 80), positive_frac=0.48, mean=-0.0015
  lin_edge.layers.2.weight: shape=(1, 80), positive_frac=0.46, mean=0.0197

======================================================================
4. CROSS-MODEL COMPARISON: What Makes 049 Different?
======================================================================

W_true statistics:
Model    mean         std          pos_frac     neg_frac    
------------------------------------------------------------
049      0.012590     0.290802     0.467        0.196       
011      0.003874     0.309673     0.370        0.237       
041      0.012930     0.330953     0.423        0.225       
003      0.006927     0.271014     0.405        0.235       

W_learned vs W_true correlation:
Model    Pearson      R2          
----------------------------------------
049      -0.1280      -0.9409     
011      -0.1225      -0.9389     
041      0.0006       -0.6797     
003      0.7726       0.5461      

======================================================================
5. EMBEDDING SPACE ANALYSIS
======================================================================

Model 049 embeddings:
  Shape: (13741, 2)
  Mean: (0.7988, 0.9477)
  Std:  (0.7329, 1.0037)
  Range X: [-1.0237, 3.0731]
  Range Y: [-3.5147, 6.0869]
  Total spread (sqrt(var_x + var_y)): 1.2428

Model 003 embeddings:
  Shape: (13741, 2)
  Mean: (1.0061, 0.8344)
  Std:  (0.7549, 0.8690)
  Range X: [-1.8490, 2.7908]
  Range Y: [-1.8948, 2.6152]
  Total spread (sqrt(var_x + var_y)): 1.1510

======================================================================
6. PER-EDGE-TYPE SIGN ANALYSIS FOR MODEL 049
======================================================================

Per-type sign inversion analysis (top 10 by edge count):
----------------------------------------------------------------------
Type   n_edges    true_pos%    learned_pos%   sign_match% 
----------------------------------------------------------------------
0      425802     64.0         45.9           25.5         INVERTED
6      930        12.3         50.3           42.6         MIXED
-6     910        10.3         58.2           35.2         MIXED
1      748        12.0         50.9           39.2         MIXED
-1     746        11.8         53.5           37.4         MIXED
2      698        12.0         50.1           39.8         MIXED
-2     696        11.8         52.2           38.1         MIXED
3      648        12.0         51.1           38.7         MIXED
-3     646        11.8         54.5           36.2         MIXED
4      598        12.0         49.5           41.8         MIXED

======================================================================
7. ALTERNATIVE APPROACHES FOR MODEL 049
======================================================================


Standard regularization (edge_norm, W_L1) FAILED for Model 049.
The sign inversion appears to be STRUCTURAL, not a regularization issue.

POTENTIAL ALTERNATIVE APPROACHES:

1. TRY lin_edge_positive=False
   - Currently, edge messages are squared (always positive)
   - This may force the GNN to use W signs incorrectly
   - Allowing negative edge messages might resolve sign degeneracy
   - CAUTION: This is a significant architectural change

2. TRY much SLOWER W learning (lr_W=1E-4)
   - Fast W learning may lock into sign-inverted local minimum
   - Slower learning might allow correct sign structure to emerge
   - Combined with stronger MLP learning to guide W

3. TRY embedding-based sign recovery
   - Increase embedding_dim to 4 or higher
   - More embedding dimensions might capture sign information
   - CAUTION: Must update input_size and input_size_update

4. TRY multi-stage training
   - Stage 1: Train only MLPs (freeze W)
   - Stage 2: Train only W (freeze MLPs)
   - May break the coupled sign inversion

5. ACCEPT FUNDAMENTAL LIMITATION
   - Model 049's low-rank activity may create true degeneracy
   - Multiple W solutions might be mathematically equivalent
   - Focus on improving tau/V_rest recovery instead

RECOMMENDATION: Try lr_W=1E-4 first (simplest change), then lin_edge_positive=False

======================================================================
8. MODEL 011: Learning Rate Embedding Sensitivity
======================================================================

Iter 14 showed lr_emb=2E-3 is CATASTROPHIC (0.716 → 0.544)

Model 011 embeddings (with lr_emb=2E-3):
  Shape: (13741, 2)
  Mean: (0.6789, 0.6117)
  Std:  (1.0725, 1.0067)
  Total spread: 1.4710

High lr_emb likely causes embeddings to become unstable,
disrupting the edge message function that depends on embeddings.

======================================================================
SUMMARY
======================================================================


MODEL STATUS AFTER BATCH 4:
  049: FAILING (0.108) - Stronger regularization made it WORSE
  011: PARTIAL (0.716 best, 0.544 current) - lr_emb=2E-3 catastrophic
  041: SOLVED (0.912) - Stable with edge_diff=1500
  003: SOLVED (0.966) - Confirmed with Iter 4 config

KEY FINDINGS:

1. MODEL 049: Regularization approach FALSIFIED
   - edge_norm=5.0 + W_L1=1E-4 made conn_R2 WORSE (0.124 → 0.108)
   - Also degraded tau_R2 and V_rest_R2
   - Sign inversion is STRUCTURAL, not fixable by regularization
   - NEED: Fundamentally different approach (lin_edge_positive=False or lr_W=1E-4)

2. MODEL 011: lr_emb constraint CONFIRMED
   - lr_emb=2E-3 destroys connectivity (0.716 → 0.544)
   - MUST stay at lr_emb=1.5E-3 or below
   - Best config remains Iter 2 (lr_W=1E-3, lr=1E-3, W_L1=3E-5)

3. MODEL 041: CONFIRMED SOLVED
   - edge_diff=1500 stable (0.912)
   - V_rest limitation (~0.01) is fundamental, not fixable

4. MODEL 003: CONFIRMED SOLVED
   - Iter 4 config is optimal (0.966)
   - No further tuning needed

NEXT BATCH RECOMMENDATIONS:
  Slot 0 (049): Try lr_W=1E-4 (very slow W learning)
  Slot 1 (011): Return to Iter 2 config, try phi_L1=0.3
  Slot 2 (041): Maintain - model solved
  Slot 3 (003): Maintain - model solved


======================================================================
END OF ANALYSIS
======================================================================
