======================================================================
ANALYSIS ITER 036: Recurrent Training Breakthrough Investigation
======================================================================

=== Batch 9 Results Summary ===
Model    Iter   conn_R2    Config                                  
-----------------------------------------------------------------
049      33     0.5010     n_layers=4, emb=4, recurrent=True       
011      34     0.5930     hidden_dim=96, n_layers=4               
041      35     0.9310     lr_W=5E-4                               
003      36     0.9620     Iter 4 baseline                         

=== W Recovery Analysis ===
Model    Pearson    R²         Sign%      MagRatio   Pos_Pear   Neg_Pear  
----------------------------------------------------------------------
049      0.6927     0.4295     80.1       113.174    0.4905     0.8301    
011      0.3336     -0.1317    65.4       284.868    0.2622     0.3779    
041      0.0122     -0.5094    51.0       84.961     -0.2112    0.2246    
003      0.8322     0.6310     84.2       54.047     0.7862     0.8486    

=== Model 049: Recurrent Training Effect ===
Iter 33 (recurrent=True):  conn_R2=0.501 (BREAKTHROUGH)
Iter 29 (recurrent=False): conn_R2=0.166
Improvement: 0.166 → 0.501 = +0.335 (3x improvement)

W recovery with recurrent_training=True:
  Pearson:    0.6927
  R²:         0.4295
  Sign match: 80.1%
  Mag ratio:  113.174
  Mean(true): 0.012590, Mean(learned): 0.022976

=== Per-Neuron W Recovery ===
Model    In_Pearson   Out_Pearson  In_Mean_T    In_Mean_L   
------------------------------------------------------------
049      0.6973       0.8309       0.3978       0.7259      
011      0.2328       0.4986       0.1224       0.6685      
041      -0.2017      0.2970       0.4085       -0.9107     
003      0.7498       0.9521       0.2188       0.4910      

=== lin_edge MLP Analysis ===

Model 049:
  Layer 0: shape=(80, 5), L2=2.416, mean_abs=0.0380, frac_large=0.117
  Layer 1: shape=(80, 80), L2=2.037, mean_abs=0.0021, frac_large=0.005
  Layer 2: shape=(80, 80), L2=2.234, mean_abs=0.0014, frac_large=0.003
  Layer 3: shape=(1, 80), L2=1.748, mean_abs=0.0463, frac_large=0.087
  Total lin_edge params: 13280

Model 011:
  Layer 0: shape=(96, 3), L2=3.433, mean_abs=0.1178, frac_large=0.375
  Layer 1: shape=(96, 96), L2=3.086, mean_abs=0.0046, frac_large=0.015
  Layer 2: shape=(96, 96), L2=2.309, mean_abs=0.0025, frac_large=0.008
  Layer 3: shape=(1, 96), L2=1.581, mean_abs=0.0616, frac_large=0.177
  Total lin_edge params: 18816

Model 041:
  Layer 0: shape=(64, 3), L2=3.853, mean_abs=0.1479, frac_large=0.359
  Layer 1: shape=(64, 64), L2=5.816, mean_abs=0.0209, frac_large=0.055
  Layer 2: shape=(1, 64), L2=1.671, mean_abs=0.0872, frac_large=0.203
  Total lin_edge params: 4352

Model 003:
  Layer 0: shape=(80, 3), L2=5.228, mean_abs=0.1661, frac_large=0.442
  Layer 1: shape=(80, 80), L2=4.948, mean_abs=0.0100, frac_large=0.033
  Layer 2: shape=(1, 80), L2=1.259, mean_abs=0.0465, frac_large=0.138
  Total lin_edge params: 6720

======================================================================
=== KEY FINDING: Why Recurrent Training Helps Model 049 ===
======================================================================

Model 049 with recurrent_training=True:
  - conn_R2 improved from 0.166 to 0.501 (3x improvement)
  - Per-neuron incoming Pearson: 0.6973
  - Per-neuron outgoing Pearson: 0.8309
  - W Pearson: 0.6927
  - Sign match: 80.1%

HYPOTHESIS: Recurrent training provides temporal context that enables:
  1. Longer-range gradient flow through time
  2. More stable W updates by seeing multiple frames in sequence
  3. Better disambiguation of degenerate W solutions

The model's low activity rank (svd_rank=19) means per-frame gradients
are weak and ambiguous. Recurrent training aggregates information
across multiple timesteps, providing stronger and more consistent
gradient signal for W learning.


=== Model 011: Why hidden_dim=96 Hurts ===

Model 011 with hidden_dim=96 (vs 80):
  - conn_R2 regressed from 0.769 to 0.593
  - Per-neuron incoming Pearson: 0.2328
  - Per-neuron outgoing Pearson: 0.4986
  - W Pearson: 0.3336

HYPOTHESIS: Excess capacity causes overfitting:
  - n_layers=4 helps by providing depth (more compositional features)
  - But hidden_dim=96 adds width (more parameters per layer)
  - Width without depth leads to overfitting/poor generalization
  - Optimal: n_layers=4 + hidden_dim=80


=== Model 041: lr_W=5E-4 is Optimal ===

Model 041 lr_W sensitivity:
  - lr_W=3E-4: conn_R2=0.888 (too slow)
  - lr_W=4E-4: conn_R2=0.919
  - lr_W=5E-4: conn_R2=0.931 (NEW BEST)
  - lr_W=6E-4: conn_R2=0.629 (baseline)

With lr_W=5E-4:
  - W Pearson: 0.0122
  - Mag ratio: 84.961

Near-collapsed activity (svd_rank=6) provides weak gradient signal.
lr_W=5E-4 is the optimal trade-off: fast enough to exploit weak signal,
slow enough to avoid overshooting.


=== Cross-Model Comparison: Status After 36 Iterations ===

| Model | Best R² | Status                    | Key Factor                        |
|-------|---------|---------------------------|-----------------------------------|
| 003   | 0.972   | FULLY SOLVED (9 conf)     | Per-neuron W correlation POSITIVE |
| 041   | 0.931   | CONNECTIVITY SOLVED       | lr_W=5E-4 optimal for low rank    |
| 011   | 0.769   | PARTIAL                   | n_layers=4 helps, width hurts     |
| 049   | 0.501*  | BREAKTHROUGH              | recurrent_training enables W      |

* Model 049 baseline was 0.634, best without recurrent was 0.181.
  With recurrent_training=True: 0.501. Path forward identified!


=== Recommendations for Batch 10 ===

1. Model 049: Try recurrent_training + edge_diff=900 + W_L1=3E-5
   - Combine recurrent breakthrough with Model 003's optimal regularization
   - Target: Exceed baseline 0.634

2. Model 011: Try recurrent_training=True
   - Test if temporal context helps like Model 049
   - If breakthrough, could improve from 0.769

3. Model 041: Maintain lr_W=5E-4 config
   - Connectivity SOLVED at 0.931
   - May try phi_L2 tuning for tau improvement

4. Model 003: Continue maintenance runs
   - FULLY SOLVED, no changes needed


======================================================================
END OF ANALYSIS
======================================================================
